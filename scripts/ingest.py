"""Script d'ingestion de documents dans le vector store."""
import asyncio
import sys
from pathlib import Path

from loguru import logger

# Ajoute le path du projet
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.config import settings
from app.core.logger import setup_logging
from app.services.chunker import get_chunker_service
from app.services.document_loader import get_document_loader
from app.services.embeddings import get_embedding_service
from app.services.vectorstore import get_vectorstore


async def ingest_documents(data_dir: Path | None = None):
    """
    Ing√®re tous les documents d'un r√©pertoire.

    Args:
        data_dir: R√©pertoire contenant les documents (par d√©faut: data/raw)
    """
    # Setup
    setup_logging()
    logger.info("=" * 60)
    logger.info("D√âBUT DE L'INGESTION")
    logger.info("=" * 60)

    data_dir = data_dir or settings.data_dir

    # Services
    loader = get_document_loader()
    chunker = get_chunker_service()
    embedding_service = get_embedding_service()
    vectorstore = get_vectorstore()

    # 1. Charge les documents
    logger.info(f"Chargement depuis : {data_dir}")
    documents = loader.load_directory(data_dir)

    if not documents:
        logger.error(f"‚ùå Aucun document trouv√© dans {data_dir}")
        return

    logger.info(f"{len(documents)} documents charg√©s")

    # 2. D√©coupe en chunks
    logger.info("üî™ D√©coupage en chunks...")
    all_chunks = []

    for file_path, content, file_type in documents:
        try:
            chunks = chunker.chunk_text(
                text=content,
                source=file_path.name,
                file_type=file_type
            )
            all_chunks.extend(chunks)
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©coupage {file_path.name} : {e}")

    if not all_chunks:
        logger.error("‚ùå Aucun chunk cr√©√©")
        return

    logger.info(f"{len(all_chunks)} chunks cr√©√©s")

    # 3. G√©n√®re les embeddings
    logger.info("G√©n√©ration des embeddings...")
    texts = [chunk.text for chunk in all_chunks]

    try:
        embeddings = await embedding_service.embed_batch(texts)

        # Attache les embeddings aux chunks
        for chunk, embedding in zip(all_chunks, embeddings):
            chunk.embedding = embedding

        logger.info(f"{len(embeddings)} embeddings g√©n√©r√©s")

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©ration embeddings : {e}")
        return

    # 4. Indexation dans FAISS
    logger.info("Indexation dans FAISS...")

    try:
        vectorstore.create_index()
        vectorstore.add_chunks(all_chunks)

        logger.info(f"{len(all_chunks)} chunks index√©s")

    except Exception as e:
        logger.error(f"‚ùå Erreur indexation : {e}")
        return

    # 5. Sauvegarde
    logger.info("Sauvegarde de l'index...")

    try:
        vectorstore.save()
        logger.info("Index sauvegard√©")

    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde : {e}")
        return

    # R√©sum√©
    logger.info("=" * 60)
    logger.info("INGESTION TERMIN√âE AVEC SUCC√àS")
    logger.info(f"üìä Documents : {len(documents)}")
    logger.info(f"üìä Chunks : {len(all_chunks)}")
    logger.info(f"üìä Index : {settings.faiss_index_full_path}")
    logger.info("=" * 60)


def main():
    """Point d'entr√©e du script."""
    # Parse arguments (optionnel)
    import argparse

    parser = argparse.ArgumentParser(description="Ingestion de documents")
    parser.add_argument(
        "--data-dir",
        type=str,
        help=f"R√©pertoire des documents (d√©faut: {settings.data_dir})"
    )

    args = parser.parse_args()
    data_dir = Path(args.data_dir) if args.data_dir else None

    # Lance l'ingestion
    asyncio.run(ingest_documents(data_dir))


if __name__ == "__main__":
    main()